\documentclass{mynotes}

\usepackage{lipsum}

\input{shortcuts}

\hfuzz=106.002pt %suppress annoying overflows


\title{ SCCGNN: any ideas? }

\author[1]{Anton Savostianov}
\author[1]{Francesco Tudisco}
\author[1]{Nicola Guglielmi}

\affil[1]{Gran Sasso Science Institute, viale F.Crispi 7, L'Aquila, Italy, 67100, email: \email{anton.savostianov@gssi.it} }

\abstract{
      \lipsum[1]
}

\keywords{NNs}

\linespread{1.15}

\begin{document}
\setlength{\abovedisplayskip}{3pt}
\setlength{\belowdisplayskip}{3pt}

      \maketitle

\section{ Things }

Let \( \mc K \) be a simplicial complex where \( \V k \) is the set of \(k\)-order simplices in \( \mc K \), \( \mc  K = \V 0 \cup \V 1 \cup \V 2 \cup \ldots\); let \( m_k = | \V k | \). Let \( B_k \in \mathrm{Mat}_{m_{k-1} \times m_k} \) be a \emph{boundary} operator mapping simplex of order \( k \) to its border of the order \( k - 1 \); by the fundamental lemma of topology \( B_k B_{k+1} = 0 \).

Given the lemma above, the \emph{homology group} \( \mc H_k = \faktor{\im B_{k}}{\ker B_{k+1}} \) is correctly defined; the elements of \( \mc H_k \) correspond to \(k\)-dimensional holes in the simplicial complex \( \mc K \). Through the harmonic representative it is convenient to exploit the isomorphism:
\begin{equation*}
      \mc H_k \cong \ker \left( B_k^\top B_k + B_{k+1} B_{k+1}^\top \right)
\end{equation*}
Operators \( \Ld k = B_k^\top B_k \), \( \Lu k = B_{k+1} B_{k+1}^\top \) and \( L_k = \Ld k + \Lu k \) are referred as \(k-\)th order up-, down- and complete graph Laplacians.

Algebra of boundary operators admits an important full space decomposition:
\begin{lemma}[Hodge Decomposition, {\cite[Thm.~5.2]{limHodgeLaplaciansGraphs2019}}]\label{lem:hodge_decomp}
      For the \(k\)-th order Hodge Laplacian \( L_k \in \ds R^{{m_k} \times {m_k}} \), the following decomposition holds:
      \begin{equation*}
            \ds R^{m_k} = \lefteqn{\overbrace{\phantom{\im B_k^\top \oplus  \ker L_k}}^{\ker B_{k+1}^\top}} \im B_k^\top \oplus
            \underbrace{\ker L_k \oplus  \im B_{k+1}}_{\ker B_k}
      \end{equation*}
\end{lemma}

We also assume the case of weighted simplicial complex; in such situations the family of \( k\) weight functions are introduced: \( w_k(\cdot) : \V k \mapsto (0; +\infty) \) with corresponding diagonal weight matrices \(W_k \in \mathrm{Mat}_{m_k \times m_k}\) where \( \left[ W_k \right]_{ii} = w_k(\sigma_i) \), \( \sigma_i \in \V k \). Then the weighting scheme preserving the homology definition can be designed as follows:
\begin{equation*}
      B_k \rightarrow W_{k-1}^{-1} B_k W_k
\end{equation*}
Weighting does not change the dimensionality of the homology group and the remaining terms in the decomposition; isomorhism and the decomposition above hold in the weighted case as well.\todo{here we need a proper discussion of the normalisation properties in the general case}


\section{ SCCGNN }


\begin{comment}
Let \( C_k \) be a formal linear space on \( \V k \).

We consider a semi-supervised setting\todo{I am not clear on terms: is it semi-supervised if not the dominant share of values is missing?}: we have the flow \( \b x \in C_1 \) with a share of missing values; the goal is to reconstruct the missing values (let \( \nu \) be the share of the missing values; missing entries in \( \b x \) are filled with the median\todo{or \( 0 \)} value of the present values).
\end{comment}

\paragraph{Graph Fourier Transform. }

Let \( L \) be a graph Laplacian (of any order); then \( L_k \) is simmetric positive semidefinite and has a complete set of orthonormal eigenvectors \( U = [ \b u_1, \b u_2, \ldots \b u_n ]\), \( L_k = U \Lambda U^\top \). Then \emph{graph Fourier transform } of a simplicial signal \( \b x \in C_k \) is defined as \( \hat{ \b x } = U^\top \b x \)\todo{exactly the same as in the discrete Fourier transform with a different basis instead of \( e^{-2\pi i k / n }\)}, \cite{kipfSemiSupervisedClassificationGraph2017}, and the convolution on graphs:
\begin{equation}
      \b x \star \b y = U \left(  \left( U^\top \b x \right)  \odot \left( U^\top \b y \right) \right) = U \diag \left( U^\top \b x \right) U^\top \b y
\end{equation}

Then graph filter \( g_\theta \) act as \( g_\theta ( L_k ) = U g_\theta ( \Lambda ) U^\top \); since \( g_\theta ( \Lambda ) \) is diagonal, every filter application \( g_\theta (L_k) \b x \) is a graph convolution. Hence, for each convolution of \( \b x \) with a kernel \( \b w \), one can find a graph filter
\begin{equation}
      \diag \left( U^\top \b w \right) = g_\theta ( \Lambda )
\end{equation}
Instead of the unpleasant and computationally demanding generic case of \( g_\theta (L_k) \), one can use approximations by the polynomial filters of a smaller degree\todo{ \( T_i \) are Chebyshev polynomials }:
\begin{equation}
      g_\theta (L_k ) \approx \sum_{i=0}^K \theta_i T_i ( \hat L_k ) \approx   \sum_{i=0}^K \alpha_i L_k^i
\end{equation}


\paragraph{ Convolutional Layers. }

Let \( \b x_i \in C_1 \); then a convolutional layer induced by the simplicial geometry is given by:
\begin{equation*}
      \b x^{(n+1)} = \sigma \left( \sum_{i=0}^K \alpha_i^{(n)} L_1^i \b x^{(n)} \right), \quad \text{ where } \sigma=\mathrm{reLU} \text{ and } \alpha_i^{(n)} \in \ds R
\end{equation*}
where \(\sigma ( x ) = x \cdot \chi(x) \). Note that \( L_1^i = \left( B_1^\top B_1 \right)^i + \left( B_2 B_2^\top \right)^i = {\Ld 1}^i + {\Lu 1}^i \); let \( P_K( \b \alpha, A ) = \sum_{i=0}^K \alpha_i A^i \) be a matrix polynomial. Then, the convolution layer can be rewritten as
\begin{equation}
      \begin{aligned}
            \b x^{(n+1)} & = \sigma \left( \sum_{i=0}^K \alpha_i^{(n)} L_1^i \b x^{(n)} \right) = \sigma \left( P_K( \b \alpha^{(n)}, L_1) \b x^{(n)} \right) = \\
            & = \sigma \left( P_K( \b \alpha^{(n)}, \Ld 1) \b x^{(n)} + P_K( \b \alpha^{(n)}, \Lu 1) \b x^{(n)}  \right) 
      \end{aligned}
\end{equation}
\begin{comment}
\begin{remark} 
      Given the classical definition, the layer produces the output vector \( \b x_{n+1} \) as an element of the Krylov\todo{why do we need it? who knows} subspaces spanned by the input \( \b x_n \) baring the activation function:
      \begin{equation*}
            \b x_n = \sigma \left( \b y_n \right), \qquad \b y_n \in \mc K_K (L_1, \b x_n)
      \end{equation*}
\end{remark}
\end{comment}
Equation above implies the same vector of polynomial coefficients for both up- and down-terms. In a more general case, one could consider a layer with two independent polynomials:
\begin{equation}
      \b x^{(n+1)} = \sigma \left( P_K( \b \alpha^{(n)}, \Ld 1) \b x^{(n)} + Q_K( \b \beta^{(n)}, \Lu 1) \b x^{(n)}  \right) 
\end{equation}
Let us omit \( \b \alpha \) and \( \b \beta \) in the polynomial arguments for clarity (we will refer to the coefficients of \( P_K \) as \( \alpha_i \) and \( Q_k \) as \( \beta_i \) respectively).

Assuming one can extend the dimension of the input for the hidden layers, \( \b x^{(n)} \in \ds R^{m_1 \times 1}\) and \( \b x^{(n+1)} \in \ds R^{m_1 \times H} \) and \( \alpha_i^{(n)}, \beta_i^{(n)} \in \ds R^{ 1 \times H } \)\todo{ the order of mutliplication changes, \( P_K(\b \alpha^{(n)}, L_1) \b x^{(n)} = \sum_{i=0}^K L_1^i \b x^{(n)} \alpha_i^{(n)}  \) }.


\paragraph{ Training, Batching, and Masking. }

Let \( \b x \in C_1 \) be a given given flow on edges. We separate vector's entries into two parts --- \emph{missing} and \emph{complete}: let \( C \subset \bar{1..m_1} \) be a set of indices for the complete entries in \( \b x \) and \( M \subset 1..m_1\) be a set of missing indices, \( C \sqcup M = 1..m_1\). For each vector \( \b x \) we denote the vector of complete entries by \( \b x_c \in \ds R^{| C | }\) and the missing vector by \(\b x_m \) accordingly, \( \b x = \begin{bmatrix} \b x_c \\ \b x_m \end{bmatrix} \) modulo the permutation of entries.

Note that the missing part of the vector is\todo{this is going to be a little shocking but bare with me} missing and is unavailible during training. Instead, we assume that the vector \( \w x \) that arrives as the input of SCCGNN is \( \w x = \begin{bmatrix} \b x_c \\ \w x_m \end{bmatrix} \) where \( \w x_m = \phi \) where \( \phi \) is a filler value for the missing entries, an \emph{initial guess}. One may opt for  \( \phi = 0 \) or \( \phi = \mathrm{median}\, \left(  \b x_c \right) \).

\begin{definition}[SCCGNN]
      \(L\)-layer \emph{SCCGNN} \( f(\w x) = \b x^{(L)}\) is defined as application of \( L \) consequetive convolutional layers with the activation function \( \sigma(\cdot )\) starting from \( \w x\):
      \begin{equation}
            \begin{aligned}
                  & \b x^{(0)} = \w x \\[-0.5\baselineskip]
                  & \b x^{(1)} = \sigma \left( P_K( \b \alpha^{(0)}, \Ld 1) \b x^{(0)} + Q_K( \b \beta^{(0)}, \Lu 1) \b x^{(0)}  \right)  \\[-0.5\baselineskip]
                  & \ldots \\[-0.5\baselineskip]
                  & \b x^{(L)} = \sigma \left( P_K( \b \alpha^{(L-1)}, \Ld 1) \b x^{(L-1)} + Q_K( \b \beta^{(L-1)}, \Lu 1) \b x^{(L-1)}  \right) 
            \end{aligned}
      \end{equation}
\end{definition}

\begin{problem}
      Train a SCCGNN \( f(\w x)\) composed of convolutional graph layers such that \( f(\w x )\) reconstructs the correct dependency on the missing values due to simplicial structure: 
      \begin{equation}
            \left\| f(\w x)_m - \b x_m  \right\|^2 \to \min
      \end{equation}
      without exploiting values from \( \b x_m \).
\end{problem}

\begin{remark}[Batching and SGD without S]
      By the nature of the task \( \b x \) maybe the only data point available; this implies that data points cannot be grouped into batches and the optimisation with SGD loses the stochasticity (so, widely used \texttt{Adam} optimizer would run deterministic and highly dependent on the initial parameter values). 

      Naturally, one may try to sample entries of \( \b x \) to form a batch (in other words, pick up a subset of the edges and disregard all the others), but this brute force idea contradicts the interconnected nature of the graph neural network. One can find a substantial discussion on the proper subsampling for GNNs (which normally mean subsampling vertices with their neighbours via specifically constructed distribution, in other words), \cite{huBatchsizeSelectionStochastic2020}, but we propose the alternative batching and randomization.

      Since the network has the access only to the known entries \( \b x_c\), it should be train to maintain correct values on the known entries (e.g. by minimizing \( \| f(\w x )_c - \b x_c \| \)) which has a global (but maybe not unique) optimum at \( f( \w x) = \w x \) (\emph{identity}). In order to avoid the trivial and incorrect sollution one may propose the following masking idea in order to simulate the same ``missing/complete'' partition on the known entries:
      \begin{itemize}[itemsep=-5pt]
            \item let \( \b e \) be a binary vector such that \( \b e_m = 0 \) and each entry of in the complete part \( \b e_c \) be \( \sim \mathrm{Bernoulli} \left( p \right) \);
            \item then \( \b x \odot \b e \) is a subsampling where we ``forget'' about some of the known entries;
            \item let us train the network with a loss:
            \begin{equation}
                  \left\| f( \w x \odot \b e )_c - \b x_c \right\|^2 
            \end{equation}
            so it avoids the identity and tries to keep the original known values intact;
            \item then the epoch is naturally formed by \( C \ln m_1 / p \) steps (such that each entry of \( \b x_c \) enters the training).
      \end{itemize}
\end{remark}


\paragraph{Generated data. } We consider the simplicial complex \( \mc K \) generated by the Delaunay triangulation of the unit square: the vertex set \( \V 0 \) is formed by the corners of the square and \( \left( m_0 - 4 \right) \) uniformly sampled points inside the square; \( \V 1 \) and \( \V 2 \) are provided by the triangulation. In order to obtain a non-trivial \( \mc H_1 \), we exclude \( 2 \) random triangles from \( \V 2 \), so \( \dim \mc H_1 = 2 \).
\begin{itemize}[itemsep=-5pt]
      \item weights of the edges chosen almost unit: \( w_1( \sigma ) = 1 + \mc N \left( 0, \frac{1}{100} \right) \), modulo the exception for the unstable complex described below;
      \item weights of the vertices are cumulitive weights of the adjacent edges: \( w_0 ( v ) = \sum_{\substack{ \sigma \in \V 1 \\ v \in \sigma }} w_1(\sigma) \);
      \item weights of the triangles are following the min-rule:
      \( w_2( \tau ) = \min_{\substack{ \sigma \in \V 1 \\ \sigma \in \tau }} w_1(\sigma)\).
\end{itemize}

We use two setups for experiments: the \emph{stable} \( \mc K \), described above, and the \emph{unstable} were two (randomly chosen) edges have a small weight \( \eps = 10^{-2} \). Defined instability agrees with both definition proposed by us in previous works: it takes a small edge perturbation to create another dimension in \( \mc H_1 \) and the conditionality of \( \Lu 1 \) and \( L_1 \) is horrifying.

The target vector \( \b x \in C_1 \) is produced as follows: we task SCCGNN to reconstruct some simple function of the local simplex structure. For instance, in the experiments below, we use the sum of the adjacent to the edge triangles:
\begin{equation}
      x_\sigma = \sum_{\substack{ \tau \in \V 2 \\ \sigma \in \tau }} w_2(\tau) + \mc N \left( 0, \frac{1}{100} \right) 
\end{equation} 



\paragraph{ Technical details. } We use \( 3 \)-layer SCCGNN with \( K = 3 \) filter size (no more than cubic polynomial degree); the share of missing values in \( \w x \) is \( \nu = 0.25 \); the filler value \( \phi \) is chosen to be the median of known values.

The complex \( \mc K \) is generated by the triangulation of \( 14 \) total nodes on the unit square; number of edges \( m_1 = 35\), number of triangles \( m_2 = 20 \). 

One epoch is composed of \( 20 \) batches, each batch has the entry presence probability of \( p = \frac{ 1 }{ 2 } \); the training lasts no longer than \( 5000 \) epochs or untill the loss falls below \( 10^{-2} \).

The loss is measured by MSE:
\begin{equation}
      \mc L = \frac{1}{ | C | } \left\| f( \w x \odot \b e )_c - \b x_c \right\|^2 
\end{equation}
The accuracy on the missing value is measure by \texttt{r2score}:
\begin{equation}
      r^2 = 1 - \frac{ \frac{1}{|M|}  \left\| f( \w x )_m - \b x_m \right\|^2   }{ \texttt{Var} \left( f( \w x )_m \right) }
\end{equation}



\paragraph{First results. } 123
















\begin{comment}

\section{Flow leakage}

For the Hodge decomposition, \Cref{lem:hodge_decomp}, we define projectors on \( \im B_1^\top \) and \( \im B_2 \) as \( \Pi_1 = B_1^\top \left( B_1 B_1^\top \right)^+ B_1 \) and \( \Pi_2 = B_2 \left( B_2^\top B_2 \right)^+ B_2^\top  \) respectively\todo{note that one can use stochastic Cholesky and \texttt{HeCS} as fast projectors here}.

Then 
\begin{equation}
      \begin{aligned}
            \b x_{n+1} & =  \sigma \left( P_K( \b \alpha^{(n)}, \Ld 1) \b x_n + Q_K( \b \beta^{(n)}, \Lu 1) \b x_n  \right) = \\
            & = \sigma \left( \gamma^{(n)} \b x_n + P_{K-1}(\Ld 1)  \Ld 1 \b x_n +Q_{K-1} (\Lu 1)  \Lu 1  \b x_n \right)
      \end{aligned}
\end{equation}
Let \( \b x_n = \b y_n + \b h_n + \b z_n\) such that \( \b y_n \in \im B_1^\top \), \( \b z_n \in \im B_2 \) and \( \b h_n \in \ker L_1 \). Then:
\begin{equation}
      \b x_n = \sigma \left( \gamma^{(n)} \b h_n + \left[ \gamma^{(n)} I + P_{K-1}(\Ld 1) \Ld 1  \right]  \b y_n + \left[ \gamma^{(n)} I + Q_{K-1}(\Lu 1) \Lu 1  \right]  \b z_n \right)
\end{equation}



\begin{remark}[reLU action of the subspaces]

      Let \( \b y  \in \im B_1^\top \); then \( \b y = B_1^\top \b x \). What can we say about \( \sigma (B_1^\top \b x ) \)? By the definition of the reLU:
      \begin{equation}
            B_1^\top \b x  = \sigma( B_1^\top \b x ) - \sigma( - B_1^\top \b x )
      \end{equation}
      Projecting both sides of this equation by mutiplying by \( \Pi_2 \) we get
      \begin{equation}
            \begin{aligned}
                  & \Pi_2 \sigma (B_1^\top \b x ) = \Pi_2 \sigma ( - B_1^\top \b x ) \\
                  & \Pi_H \sigma (B_1^\top \b x ) = \Pi_H \sigma ( - B_1^\top \b x )
            \end{aligned}
      \end{equation}
\end{remark}

\end{comment}



































\vspace{50pt}

      \nocite{*}
      \bibliographystyle{alpha}
      \bibliography{sccgnn}

\end{document}